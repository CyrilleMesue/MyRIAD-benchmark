{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5246a625-a767-4cef-bedb-7323d1d7a97e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0bd2e3-299d-475e-a7c0-81d4d9c5f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from modules.utils import load_json, save_json\n",
    "from tqdm import tqdm  \n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd780751-c420-46c0-b3f0-3d717e406df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_view_column_splits(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Dynamically split columns into groups of 200.\n",
    "    Adapts automatically for datasets with 201, 401, or 601 columns.\n",
    "    \"\"\"\n",
    "    view_column_splits = {}\n",
    "    step = 200  # size per chunk\n",
    "\n",
    "    # Start from column index 1 to skip 'Sample ID' or first column\n",
    "    total_cols = data.shape[1] - 1\n",
    "    num_groups = (total_cols + step - 1) // step  # ceiling division\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        start = 1 + i * step\n",
    "        end = min(1 + (i + 1) * step, data.shape[1])\n",
    "        view_column_splits[i + 1] = data.columns[start:end]\n",
    "\n",
    "    return view_column_splits \n",
    "\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[mkdir] ensured: {path}\")\n",
    "\n",
    "def run_pipeline(dataset_name: str, experiment_name: str, view_column_splits: dict = None):\n",
    "    \"\"\"\n",
    "    Pipeline that:\n",
    "      - Loads baseline JSON results and converts to a DataFrame\n",
    "      - Loads post-feature-selection cv results CSV\n",
    "      - Concatenates baseline + fs results, shortens featureSelector names, saves combined CSV\n",
    "      - Copies ranks and validation CSVs into results folder, normalizes names, augments validation with ML metrics\n",
    "      - Saves val_results and small JSON view mapping\n",
    "    \n",
    "    Inputs:\n",
    "      - dataset_name (str)\n",
    "      - experiment_name (str)\n",
    "      - view_column_splits (dict) optional (if not provided an empty dict will be used)\n",
    "    \"\"\"\n",
    "    view_column_splits = view_column_splits or {}\n",
    "    base_bench = Path(f\"../BENCHMARKING/{dataset_name}/{experiment_name}\")\n",
    "    out_dir = Path(f\"../results/{dataset_name}/{experiment_name}\")\n",
    "    ensure_dir(out_dir)\n",
    "    \n",
    "    print(f\"[run_pipeline] dataset={dataset_name}, experiment={experiment_name}\")\n",
    "    # --- Load Baseline Performance ---\n",
    "    baseline_path = base_bench / \"ML-Baseline.json\"\n",
    "    print(f\"[run_pipeline] loading baseline from: {baseline_path}\")\n",
    "    baseline = load_json(baseline_path)\n",
    "    \n",
    "    baseline_results = {\n",
    "        'featureSelector': [],\n",
    "        'modelName': [], \n",
    "        'numFeatures': [],  \n",
    "        'MeanAccuracy': [],\n",
    "        'StdAccuracy': [], \n",
    "        'MeanPrecision': [], \n",
    "        'StdPrecision':[], \n",
    "        'MeanRecall':[], \n",
    "        'StdRecall': [], \n",
    "        'MeanF1': [], \n",
    "        'StdF1': [], \n",
    "        'MeanAUC': [],\n",
    "        'StdAUC':[],\n",
    "        \"MeanSpecificity\": [],\n",
    "        \"StdSpecificity\": [], \n",
    "        \"MeanNPV\": [],\n",
    "        \"StdNPV\": [],\n",
    "        \"MeanLR_PLUS\": [], \n",
    "        \"StdLR_PLUS\": [],\n",
    "        \"MeanLR_MINUS\": [],\n",
    "        \"StdLR_MINUS\": []\n",
    "    }\n",
    "    \n",
    "    if baseline:\n",
    "        for modelname, model_data in baseline.items(): \n",
    "            baseline_results['featureSelector'].append('NONE') \n",
    "            baseline_results['modelName'].append(modelname)\n",
    "            baseline_results['numFeatures'].append(600)\n",
    "            \n",
    "            if modelname in ['MORE', 'MOGONET']:\n",
    "                src = model_data  # top-level keys for these models\n",
    "            else:\n",
    "                src = model_data.get('cross_val_report', {})  # nested for other models\n",
    "            \n",
    "            # now read everything from src (safe .get with defaults)\n",
    "            baseline_results['MeanAccuracy'].append(src.get('accuracy', {}).get('mean'))\n",
    "            baseline_results['StdAccuracy'].append(src.get('accuracy', {}).get('std'))\n",
    "            baseline_results['MeanPrecision'].append(src.get('precision', {}).get('mean'))\n",
    "            baseline_results['StdPrecision'].append(src.get('precision', {}).get('std'))\n",
    "            baseline_results['MeanRecall'].append(src.get('recall', {}).get('mean'))\n",
    "            baseline_results['StdRecall'].append(src.get('recall', {}).get('std'))\n",
    "            baseline_results['MeanF1'].append(src.get('f1', {}).get('mean'))\n",
    "            baseline_results['StdF1'].append(src.get('f1', {}).get('std'))\n",
    "            baseline_results['MeanAUC'].append(src.get('roc_auc', {}).get('mean'))\n",
    "            baseline_results['StdAUC'].append(src.get('roc_auc', {}).get('std'))\n",
    "            baseline_results['MeanSpecificity'].append(src.get('specificity', {}).get('mean'))\n",
    "            baseline_results['StdSpecificity'].append(src.get('specificity', {}).get('std'))\n",
    "            baseline_results['MeanNPV'].append(src.get('npv', {}).get('mean'))\n",
    "            baseline_results['StdNPV'].append(src.get('npv', {}).get('std'))\n",
    "            baseline_results['MeanLR_PLUS'].append(src.get('lr_plus', {}).get('mean'))\n",
    "            baseline_results['StdLR_PLUS'].append(src.get('lr_plus', {}).get('std'))\n",
    "            baseline_results['MeanLR_MINUS'].append(src.get('lr_minus', {}).get('mean'))\n",
    "            baseline_results['StdLR_MINUS'].append(src.get('lr_minus', {}).get('std'))\n",
    "\n",
    "        baseline_results_df = pd.DataFrame(baseline_results)\n",
    "        print(f\"[run_pipeline] baseline -> DataFrame with shape {baseline_results_df.shape}\")\n",
    "    else:\n",
    "        baseline_results_df = pd.DataFrame(baseline_results)\n",
    "        print(\"[run_pipeline] baseline empty -> created empty baseline DataFrame\")\n",
    "    \n",
    "    # --- Load Performance after feature selection ---\n",
    "    fs_cv_path = base_bench / \"cross-validation-results.csv\"\n",
    "    print(f\"[run_pipeline] loading feature-selection CV results from: {fs_cv_path}\")\n",
    "    if fs_cv_path.exists():\n",
    "        fs_performance = pd.read_csv(fs_cv_path)\n",
    "        print(f\"[run_pipeline] loaded fs_performance shape={fs_performance.shape}\")\n",
    "    else:\n",
    "        fs_performance = pd.DataFrame()\n",
    "        print(f\"[run_pipeline] WARNING: {fs_cv_path} not found. Using empty DataFrame for fs_performance.\")\n",
    "    \n",
    "    # --- combine performance ---\n",
    "    cv_performance = pd.concat([baseline_results_df, fs_performance], axis=0, ignore_index=True, sort=False)\n",
    "    print(f\"[run_pipeline] combined cv_performance shape={cv_performance.shape}\")\n",
    "    \n",
    "    # Shorten long names\n",
    "    map_long_names = {\n",
    "        'randomforest_feature_importance': 'RF-FI',\n",
    "        'xgb_feature_importance': 'XGB-FI',\n",
    "        'rf_permutation_feature_importance': 'RF-PFI',\n",
    "        'xgb_permutation_feature_importance': 'XGB-PFI'\n",
    "    }\n",
    "    if 'featureSelector' in cv_performance.columns:\n",
    "        cv_performance['featureSelector'] = cv_performance['featureSelector'].apply(lambda x: map_long_names[x] if x in map_long_names else x)\n",
    "    \n",
    "    cv_out_path = out_dir / \"cross-validation-results.csv\"\n",
    "    cv_performance.to_csv(cv_out_path, index=False)\n",
    "    print(f\"[run_pipeline] saved combined CV performance to: {cv_out_path}\")\n",
    "    \n",
    "    # --- copy Ranks ---\n",
    "    ranks_in = base_bench / \"BiomarkerRanks.csv\"\n",
    "    ranks_out = out_dir / \"BiomarkerRanks.csv\"\n",
    "    if ranks_in.exists():\n",
    "        ranks = pd.read_csv(ranks_in)\n",
    "        ranks.to_csv(ranks_out, index=False)\n",
    "        print(f\"[run_pipeline] copied ranks to: {ranks_out} (shape={ranks.shape})\")\n",
    "    else:\n",
    "        print(f\"[run_pipeline] WARNING: ranks file not found at {ranks_in}.\")\n",
    "    \n",
    "    # --- validation results ---\n",
    "    val_in = base_bench / \"Biomarker-validation-results.csv\"\n",
    "    if val_in.exists():\n",
    "        val_results = pd.read_csv(val_in)\n",
    "        # normalize names and columns\n",
    "        val_results = val_results.rename(columns={\"Method\": \"featureSelector\"}) if \"Method\" in val_results.columns else val_results\n",
    "        if 'featureSelector' in val_results.columns:\n",
    "            val_results['featureSelector'] = val_results['featureSelector'].apply(lambda x: map_long_names[x] if x in map_long_names else x)\n",
    "        # Save a cleaned copy to results folder\n",
    "        val_out = out_dir / \"Biomarker-validation-results.csv\"\n",
    "        val_results.to_csv(val_out, index=False)\n",
    "        print(f\"[run_pipeline] saved validation results to: {val_out} (shape={val_results.shape})\")\n",
    "    else:\n",
    "        val_results = pd.DataFrame()\n",
    "        print(f\"[run_pipeline] WARNING: validation results not found at {val_in}.\")\n",
    "    \n",
    "    # --- Add ML Performance to validation results (full_val_results) ---\n",
    "    full_val_results = pd.DataFrame()\n",
    "    if not cv_performance.empty and not val_results.empty:\n",
    "        # columns to merge from cv_performance (drop featureSelector and numFeatures)\n",
    "        cv_columns = [c for c in cv_performance.columns if c not in (\"featureSelector\", \"numFeatures\")]\n",
    "        print(f\"[run_pipeline] preparing to augment validation with cv columns: {cv_columns}\")\n",
    "        \n",
    "        # Ensure numeric numFeatures in cv_performance\n",
    "        if 'numFeatures' in cv_performance.columns:\n",
    "            # If numFeatures read as float, convert to int where appropriate\n",
    "            try:\n",
    "                cv_performance['numFeatures'] = cv_performance['numFeatures'].astype(int)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        for i in range(val_results.shape[0]):\n",
    "            selector = val_results.at[i, 'featureSelector'] if 'featureSelector' in val_results.columns else None\n",
    "            num_features = val_results.at[i, 'method_cutoff'] if 'method_cutoff' in val_results.columns else None\n",
    "            \n",
    "            # only process expected cutoffs\n",
    "            if num_features in range(10, 101, 10) and selector is not None:\n",
    "                df2 = cv_performance.loc[\n",
    "                    (cv_performance['featureSelector'] == selector) & \n",
    "                    (cv_performance['numFeatures'] == num_features),\n",
    "                    cv_columns\n",
    "                ].copy()\n",
    "                \n",
    "                if df2.empty:\n",
    "                    # no matching rows found\n",
    "                    continue\n",
    "                \n",
    "                # rename Mean -> Model (as in original)\n",
    "                df2 = df2.rename(columns={col: col.replace(\"Mean\", \"Model\") for col in df2.columns})\n",
    "                \n",
    "                # duplicate the validation row to match df2 number of rows\n",
    "                row = val_results.iloc[i:i+1, :].reset_index(drop=True)\n",
    "                repeated_row = pd.concat([row]*df2.shape[0], ignore_index=True).reset_index(drop=True)\n",
    "                \n",
    "                df_chunk = pd.concat([repeated_row.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "                full_val_results = pd.concat([full_val_results, df_chunk], axis=0, ignore_index=True, sort=False)\n",
    "        \n",
    "        # final tidy\n",
    "        full_val_results = full_val_results.rename(columns={\"method_cutoff\": \"numFeatures\"}) if not full_val_results.empty else full_val_results\n",
    "        # drop fully empty columns/rows\n",
    "        full_val_results = full_val_results.T.dropna(how='all').T\n",
    "        val_results_out = out_dir / \"val_results.csv\"\n",
    "        full_val_results.to_csv(val_results_out, index=False)\n",
    "        print(f\"[run_pipeline] saved augmented validation results to: {val_results_out} (shape={full_val_results.shape})\")\n",
    "    else:\n",
    "        print(\"[run_pipeline] Skipping ML augmentation: cv_performance or val_results is empty.\")\n",
    "    \n",
    "    # --- save view_column_splits ---\n",
    "    vjson_out = out_dir / \"featurenames.json\"\n",
    "    save_json(vjson_out, {k: list(v) for k, v in (view_column_splits or {}).items()})\n",
    "    \n",
    "    # --- final prints ---\n",
    "    if 'validationsource' in val_results.columns:\n",
    "        try:\n",
    "            counts = val_results['validationsource'].value_counts()\n",
    "            print(\"[run_pipeline] validation source counts:\")\n",
    "            print(counts.to_string())\n",
    "        except Exception as e:\n",
    "            print(f\"[run_pipeline] could not print validationsource counts: {e}\")\n",
    "    else:\n",
    "        print(\"[run_pipeline] 'validationsource' column not present in validation results (no counts printed).\")\n",
    "    \n",
    "    print(\"[run_pipeline] finished.\")\n",
    "    # return a dict of produced DataFrames for convenience if the caller wants them\n",
    "    return {\n",
    "        \"cv_performance\": cv_performance,\n",
    "        \"baseline_results_df\": baseline_results_df,\n",
    "        \"ranks\": (ranks if 'ranks' in locals() else pd.DataFrame()),\n",
    "        \"val_results\": (val_results if 'val_results' in locals() else pd.DataFrame()),\n",
    "        \"full_val_results\": (full_val_results if 'full_val_results' in locals() else pd.DataFrame()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667c5d-a305-4c35-a86a-d28a4f61c2a3",
   "metadata": {},
   "source": [
    "### Load and Prepare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef06aa8-b1fc-4849-94a6-ce7a4e7b2642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mkdir] ensured: ../results/ROSMAP/miRNA_data\n",
      "[run_pipeline] dataset=ROSMAP, experiment=miRNA_data\n",
      "[run_pipeline] loading baseline from: ../BENCHMARKING/ROSMAP/miRNA_data/ML-Baseline.json\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment_name \u001b[38;5;129;01min\u001b[39;00m experiment_list:\n\u001b[1;32m     33\u001b[0m     view_column_splits \u001b[38;5;241m=\u001b[39m get_view_column_splits(df) \n\u001b[0;32m---> 34\u001b[0m     run_pipeline(dataset_name, experiment_name, view_column_splits) \n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 112\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(dataset_name, experiment_name, view_column_splits)\u001b[0m\n\u001b[1;32m    110\u001b[0m baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeanAUC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(cross\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    111\u001b[0m baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStdAUC\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(cross\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m)) \n\u001b[0;32m--> 112\u001b[0m baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeanSpecificity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_val_report\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecificity\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    113\u001b[0m baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStdSpecificity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_val_report\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecificity\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    114\u001b[0m baseline_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeanNPV\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(metric[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross_val_report\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpv\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metric' is not defined"
     ]
    }
   ],
   "source": [
    "# set dataset name\n",
    "experimental_designs = {\"ROSMAP\":['miRNA_data',\n",
    "                                  'dna_methylation_data',\n",
    "                                  'gene_expression_data',\n",
    "                                  'miRNA_and_gene_expression_data',\n",
    "                                  'miRNA_and_dna_methylation_data',\n",
    "                                  'gene_expression_and_dna_methylation_data',\n",
    "                                  'miRNA_and_gene_expression_and_dna_methylation_data'\n",
    "                                 ],\n",
    "                        'MayoRNASeq':[\n",
    "                            'metabolomics_data',\n",
    "                            'gene_expression_data',\n",
    "                            'proteomics_data',\n",
    "                            'gene_expression_and_proteomics_data',\n",
    "                            'metabolomics_and_gene_expression_data',\n",
    "                            'metabolomics_and_proteomics_data',\n",
    "                            'metabolomics_and_gene_expression_and_proteomics_data'\n",
    "                            \n",
    "                        ],\n",
    "                        'BRCA':['miRNA_data',\n",
    "                                  'dna_methylation_data',\n",
    "                                  'gene_expression_data',\n",
    "                                  'miRNA_and_gene_expression_data',\n",
    "                                  'miRNA_and_dna_methylation_data',\n",
    "                                  'gene_expression_and_dna_methylation_data',\n",
    "                                  'miRNA_and_gene_expression_and_dna_methylation_data'\n",
    "                                 ]\n",
    "                       }  \n",
    "for dataset_name, experiment_list in experimental_designs.items():\n",
    "    df = pd.read_csv(f'../data/{dataset_name}/prepared/{experiment_list[-1]}.csv', index_col=0) \n",
    "    \n",
    "    for experiment_name in experiment_list:\n",
    "        view_column_splits = get_view_column_splits(df) \n",
    "        run_pipeline(dataset_name, experiment_name, view_column_splits) \n",
    "        print('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

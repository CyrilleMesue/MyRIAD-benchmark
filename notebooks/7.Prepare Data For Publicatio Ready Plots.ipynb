{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5246a625-a767-4cef-bedb-7323d1d7a97e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0bd2e3-299d-475e-a7c0-81d4d9c5f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from modules.utils import load_json, save_json\n",
    "from tqdm import tqdm  \n",
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd780751-c420-46c0-b3f0-3d717e406df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_view_column_splits(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Dynamically split columns into groups of 200.\n",
    "    Adapts automatically for datasets with 201, 401, or 601 columns.\n",
    "    \"\"\"\n",
    "    view_column_splits = {}\n",
    "    step = 200  # size per chunk\n",
    "\n",
    "    # Start from column index 1 to skip 'Sample ID' or first column\n",
    "    total_cols = data.shape[1] - 1\n",
    "    num_groups = (total_cols + step - 1) // step  # ceiling division\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        start = 1 + i * step\n",
    "        end = min(1 + (i + 1) * step, data.shape[1])\n",
    "        view_column_splits[i + 1] = data.columns[start:end]\n",
    "\n",
    "    return view_column_splits \n",
    "\n",
    "def run_pipeline(dataset_name: str, experiment_name: str, view_column_splits: dict = None):\n",
    "    \"\"\"\n",
    "    Pipeline that:\n",
    "      - Loads baseline JSON results and converts to a DataFrame\n",
    "      - Loads post-feature-selection cv results CSV\n",
    "      - Concatenates baseline + fs results, shortens featureSelector names, saves combined CSV\n",
    "      - Copies ranks and validation CSVs into results folder, normalizes names, augments validation with ML metrics\n",
    "      - Saves val_results and small JSON view mapping\n",
    "    \n",
    "    Inputs:\n",
    "      - dataset_name (str)\n",
    "      - experiment_name (str)\n",
    "      - view_column_splits (dict) optional (if not provided an empty dict will be used)\n",
    "    \"\"\"\n",
    "    view_column_splits = view_column_splits or {}\n",
    "    base_bench = Path(f\"../BENCHMARKING/{dataset_name}/{experiment_name}\")\n",
    "    \n",
    "    # --- Load Baseline Performance ---\n",
    "    baseline_path = base_bench / \"ML-Baseline.json\"\n",
    "    baseline = load_json(baseline_path)\n",
    "    \n",
    "    baseline_results = {\n",
    "        'featureSelector': [],\n",
    "        'modelName': [], \n",
    "        'numFeatures': [],  \n",
    "        'MeanAccuracy': [],\n",
    "        'StdAccuracy': [], \n",
    "        'MeanPrecision': [], \n",
    "        'StdPrecision':[], \n",
    "        'MeanRecall':[], \n",
    "        'StdRecall': [], \n",
    "        'MeanF1': [], \n",
    "        'StdF1': [], \n",
    "        'MeanAUC': [],\n",
    "        'StdAUC':[],\n",
    "        \"MeanSpecificity\": [],\n",
    "        \"StdSpecificity\": [], \n",
    "        \"MeanNPV\": [],\n",
    "        \"StdNPV\": [],\n",
    "        \"MeanLR_PLUS\": [], \n",
    "        \"StdLR_PLUS\": [],\n",
    "        \"MeanLR_MINUS\": [],\n",
    "        \"StdLR_MINUS\": []\n",
    "    }\n",
    "    \n",
    "    if baseline:\n",
    "        for modelname, model_data in baseline.items(): \n",
    "            baseline_results['featureSelector'].append('NONE') \n",
    "            baseline_results['modelName'].append(modelname)\n",
    "            baseline_results['numFeatures'].append(600)\n",
    "            \n",
    "            if modelname in ['MORE', 'MOGONET']:\n",
    "                src = model_data  # top-level keys for these models\n",
    "            else:\n",
    "                src = model_data.get('cross_val_report', {})  # nested for other models\n",
    "            \n",
    "            # now read everything from src (safe .get with defaults)\n",
    "            baseline_results['MeanAccuracy'].append(src.get('accuracy', {}).get('mean'))\n",
    "            baseline_results['StdAccuracy'].append(src.get('accuracy', {}).get('std'))\n",
    "            baseline_results['MeanPrecision'].append(src.get('precision', {}).get('mean'))\n",
    "            baseline_results['StdPrecision'].append(src.get('precision', {}).get('std'))\n",
    "            baseline_results['MeanRecall'].append(src.get('recall', {}).get('mean'))\n",
    "            baseline_results['StdRecall'].append(src.get('recall', {}).get('std'))\n",
    "            baseline_results['MeanF1'].append(src.get('f1', {}).get('mean'))\n",
    "            baseline_results['StdF1'].append(src.get('f1', {}).get('std'))\n",
    "            baseline_results['MeanAUC'].append(src.get('roc_auc', {}).get('mean'))\n",
    "            baseline_results['StdAUC'].append(src.get('roc_auc', {}).get('std'))\n",
    "            baseline_results['MeanSpecificity'].append(src.get('specificity', {}).get('mean'))\n",
    "            baseline_results['StdSpecificity'].append(src.get('specificity', {}).get('std'))\n",
    "            baseline_results['MeanNPV'].append(src.get('npv', {}).get('mean'))\n",
    "            baseline_results['StdNPV'].append(src.get('npv', {}).get('std'))\n",
    "            baseline_results['MeanLR_PLUS'].append(src.get('lr_plus', {}).get('mean'))\n",
    "            baseline_results['StdLR_PLUS'].append(src.get('lr_plus', {}).get('std'))\n",
    "            baseline_results['MeanLR_MINUS'].append(src.get('lr_minus', {}).get('mean'))\n",
    "            baseline_results['StdLR_MINUS'].append(src.get('lr_minus', {}).get('std'))\n",
    "\n",
    "        baseline_results_df = pd.DataFrame(baseline_results)\n",
    "    else:\n",
    "        baseline_results_df = pd.DataFrame(baseline_results)\n",
    "        print(\"[run_pipeline] baseline empty -> created empty baseline DataFrame\")\n",
    "    \n",
    "    # --- Load Performance after feature selection ---\n",
    "    fs_cv_path = base_bench / \"cross-validation-results.csv\"\n",
    "    if fs_cv_path.exists():\n",
    "        fs_performance = pd.read_csv(fs_cv_path)\n",
    "\n",
    "    else:\n",
    "        fs_performance = pd.DataFrame()\n",
    "        print(f\"[run_pipeline] WARNING: {fs_cv_path} not found. Using empty DataFrame for fs_performance.\")\n",
    "    \n",
    "    # --- combine performance ---\n",
    "    cv_performance = pd.concat([baseline_results_df, fs_performance], axis=0, ignore_index=True, sort=False)\n",
    "    \n",
    "    # Shorten long names\n",
    "    map_long_names = {\n",
    "        'randomforest_feature_importance': 'RF-FI',\n",
    "        'xgb_feature_importance': 'XGB-FI',\n",
    "        'rf_permutation_feature_importance': 'RF-PFI',\n",
    "        'xgb_permutation_feature_importance': 'XGB-PFI'\n",
    "    }\n",
    "    if 'featureSelector' in cv_performance.columns:\n",
    "        cv_performance['featureSelector'] = cv_performance['featureSelector'].apply(lambda x: map_long_names[x] if x in map_long_names else x)\n",
    "    \n",
    "    # --- copy Ranks ---\n",
    "    ranks_in = base_bench / \"BiomarkerRanks.csv\"\n",
    "\n",
    "    if ranks_in.exists():\n",
    "        ranks = pd.read_csv(ranks_in)\n",
    "    else:\n",
    "        print(f\"[run_pipeline] WARNING: ranks file not found at {ranks_in}.\")\n",
    "    \n",
    "    # --- validation results ---\n",
    "    val_in = base_bench / \"Biomarker-validation-results.csv\"\n",
    "    if val_in.exists():\n",
    "        val_results = pd.read_csv(val_in)\n",
    "        # normalize names and columns\n",
    "        val_results = val_results.rename(columns={\"Method\": \"featureSelector\"}) if \"Method\" in val_results.columns else val_results\n",
    "        if 'featureSelector' in val_results.columns:\n",
    "            val_results['featureSelector'] = val_results['featureSelector'].apply(lambda x: map_long_names[x] if x in map_long_names else x)\n",
    "    else:\n",
    "        val_results = pd.DataFrame()\n",
    "    \n",
    "    # --- Add ML Performance to validation results (full_val_results) ---\n",
    "    full_val_results = pd.DataFrame()\n",
    "    if not cv_performance.empty and not val_results.empty:\n",
    "        # columns to merge from cv_performance (drop featureSelector and numFeatures)\n",
    "        cv_columns = [c for c in cv_performance.columns if c not in (\"featureSelector\", \"numFeatures\")]\n",
    "        \n",
    "        # Ensure numeric numFeatures in cv_performance\n",
    "        if 'numFeatures' in cv_performance.columns:\n",
    "            # If numFeatures read as float, convert to int where appropriate\n",
    "            try:\n",
    "                cv_performance['numFeatures'] = cv_performance['numFeatures'].astype(int)\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        for i in range(val_results.shape[0]):\n",
    "            selector = val_results.at[i, 'featureSelector'] if 'featureSelector' in val_results.columns else None\n",
    "            num_features = val_results.at[i, 'method_cutoff'] if 'method_cutoff' in val_results.columns else None\n",
    "            \n",
    "            # only process expected cutoffs\n",
    "            if num_features in range(10, 101, 10) and selector is not None:\n",
    "                df2 = cv_performance.loc[\n",
    "                    (cv_performance['featureSelector'] == selector) & \n",
    "                    (cv_performance['numFeatures'] == num_features),\n",
    "                    cv_columns\n",
    "                ].copy()\n",
    "                \n",
    "                if df2.empty:\n",
    "                    # no matching rows found\n",
    "                    continue\n",
    "                \n",
    "                # rename Mean -> Model (as in original)\n",
    "                df2 = df2.rename(columns={col: col.replace(\"Mean\", \"Model\") for col in df2.columns})\n",
    "                \n",
    "                # duplicate the validation row to match df2 number of rows\n",
    "                row = val_results.iloc[i:i+1, :].reset_index(drop=True)\n",
    "                repeated_row = pd.concat([row]*df2.shape[0], ignore_index=True).reset_index(drop=True)\n",
    "                \n",
    "                df_chunk = pd.concat([repeated_row.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "                full_val_results = pd.concat([full_val_results, df_chunk], axis=0, ignore_index=True, sort=False)\n",
    "        \n",
    "        # final tidy\n",
    "        full_val_results = full_val_results.rename(columns={\"method_cutoff\": \"numFeatures\"}) if not full_val_results.empty else full_val_results\n",
    "        # drop fully empty columns/rows\n",
    "        full_val_results = full_val_results.T.dropna(how='all').T\n",
    "    else:\n",
    "        print(\"[run_pipeline] Skipping ML augmentation: cv_performance or val_results is empty.\")\n",
    "    \n",
    "    # --- final prints ---\n",
    "    if 'validationsource' in val_results.columns:\n",
    "        try:\n",
    "            counts = val_results['validationsource'].value_counts()\n",
    "        except Exception as e:\n",
    "            print(f\"[run_pipeline] could not print validationsource counts: {e}\")\n",
    "    else:\n",
    "        print(\"[run_pipeline] 'validationsource' column not present in validation results (no counts printed).\")\n",
    "    \n",
    "    # return a dict of produced DataFrames for convenience if the caller wants them\n",
    "    return {\n",
    "        \"cv_performance\": cv_performance,\n",
    "        \"baseline_results_df\": baseline_results_df,\n",
    "        \"ranks\": (ranks if 'ranks' in locals() else pd.DataFrame()),\n",
    "        \"val_results\": (val_results if 'val_results' in locals() else pd.DataFrame()),\n",
    "        \"full_val_results\": (full_val_results if 'full_val_results' in locals() else pd.DataFrame()),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b222d8c-c25c-4ffd-ad54-f6e41f409120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rank_matrix(ranks_df, fill_missing=None, one_based=True):\n",
    "    \"\"\"\n",
    "    Transform a wide rank DataFrame (methods as columns, ranked features as rows)\n",
    "    into a rank matrix where rows are features, columns are methods, \n",
    "    and cells contain the rank of the feature under that method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ranks_df : pd.DataFrame\n",
    "        Input DataFrame, each column is a method, each row is a rank position\n",
    "        containing the feature selected at that rank.\n",
    "    fill_missing : int or None, optional\n",
    "        Value to fill in for features not ranked by a method.\n",
    "        - If None: leaves NaN\n",
    "        - If int: fills with that value\n",
    "        - If \"max+1\": fills with (number of rows + 1), i.e., worst rank\n",
    "    one_based : bool, default=True\n",
    "        If True, ranks are 1-based (1, 2, …). If False, ranks are 0-based (0, 1, …).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rank_matrix : pd.DataFrame\n",
    "        Features × Methods DataFrame of ranks.\n",
    "    \"\"\"\n",
    "    # Melt into long form with explicit rank positions\n",
    "    df_long = (\n",
    "        ranks_df\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'pos'})\n",
    "        .melt(id_vars='pos', var_name='featureSelector', value_name='feature')\n",
    "        .dropna(subset=['feature'])\n",
    "    )\n",
    "    \n",
    "    # Compute rank\n",
    "    df_long['rank'] = df_long['pos'] + (1 if one_based else 0)\n",
    "\n",
    "    # Pivot to feature × method\n",
    "    rank_matrix = df_long.pivot_table(\n",
    "        index='feature',\n",
    "        columns='featureSelector',\n",
    "        values='rank',\n",
    "        aggfunc='min'\n",
    "    )\n",
    "\n",
    "    # Reindex to include all features observed\n",
    "    all_features = pd.Index(pd.unique(ranks_df.values.ravel())).dropna()\n",
    "    rank_matrix = rank_matrix.reindex(all_features)\n",
    "\n",
    "    # Fill missing if requested\n",
    "    if fill_missing is not None:\n",
    "        if fill_missing == \"max+1\":\n",
    "            fill_value = ranks_df.shape[0] + (1 if one_based else 0)\n",
    "        else:\n",
    "            fill_value = fill_missing\n",
    "        rank_matrix = rank_matrix.fillna(fill_value).astype(int)\n",
    "\n",
    "    return rank_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667c5d-a305-4c35-a86a-d28a4f61c2a3",
   "metadata": {},
   "source": [
    "### Load and Prepare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef06aa8-b1fc-4849-94a6-ce7a4e7b2642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORKING ON ROSMAP DATASET\n",
      "WORKING ON miRNA_data EXPERIMENT\n",
      "WORKING ON dna_methylation_data EXPERIMENT\n",
      "[run_pipeline] Skipping ML augmentation: cv_performance or val_results is empty.\n",
      "WORKING ON gene_expression_data EXPERIMENT\n",
      "WORKING ON miRNA_and_gene_expression_data EXPERIMENT\n",
      "WORKING ON miRNA_and_dna_methylation_data EXPERIMENT\n",
      "WORKING ON gene_expression_and_dna_methylation_data EXPERIMENT\n",
      "WORKING ON miRNA_and_gene_expression_and_dna_methylation_data EXPERIMENT\n",
      "\n",
      "WORKING ON MayoRNASeq DATASET\n",
      "WORKING ON metabolomics_data EXPERIMENT\n",
      "[run_pipeline] Skipping ML augmentation: cv_performance or val_results is empty.\n",
      "WORKING ON gene_expression_data EXPERIMENT\n",
      "WORKING ON proteomics_data EXPERIMENT\n",
      "WORKING ON gene_expression_and_proteomics_data EXPERIMENT\n",
      "WORKING ON metabolomics_and_gene_expression_data EXPERIMENT\n",
      "WORKING ON metabolomics_and_proteomics_data EXPERIMENT\n",
      "WORKING ON metabolomics_and_gene_expression_and_proteomics_data EXPERIMENT\n",
      "\n",
      "WORKING ON BRCA DATASET\n",
      "WORKING ON miRNA_data EXPERIMENT\n",
      "WORKING ON dna_methylation_data EXPERIMENT\n",
      "WORKING ON gene_expression_data EXPERIMENT\n",
      "WORKING ON miRNA_and_gene_expression_data EXPERIMENT\n",
      "WORKING ON miRNA_and_dna_methylation_data EXPERIMENT\n",
      "WORKING ON gene_expression_and_dna_methylation_data EXPERIMENT\n",
      "WORKING ON miRNA_and_gene_expression_and_dna_methylation_data EXPERIMENT\n"
     ]
    }
   ],
   "source": [
    "# set dataset name\n",
    "experimental_designs = {\"ROSMAP\":['miRNA_data',\n",
    "                                  'dna_methylation_data',\n",
    "                                  'gene_expression_data',\n",
    "                                  'miRNA_and_gene_expression_data',\n",
    "                                  'miRNA_and_dna_methylation_data',\n",
    "                                  'gene_expression_and_dna_methylation_data',\n",
    "                                  'miRNA_and_gene_expression_and_dna_methylation_data'\n",
    "                                 ],\n",
    "                        'MayoRNASeq':[\n",
    "                            'metabolomics_data',\n",
    "                            'gene_expression_data',\n",
    "                            'proteomics_data',\n",
    "                            'gene_expression_and_proteomics_data',\n",
    "                            'metabolomics_and_gene_expression_data',\n",
    "                            'metabolomics_and_proteomics_data',\n",
    "                            'metabolomics_and_gene_expression_and_proteomics_data'\n",
    "                            \n",
    "                        ],\n",
    "                        'BRCA':['miRNA_data',\n",
    "                                  'dna_methylation_data',\n",
    "                                  'gene_expression_data',\n",
    "                                  'miRNA_and_gene_expression_data',\n",
    "                                  'miRNA_and_dna_methylation_data',\n",
    "                                  'gene_expression_and_dna_methylation_data',\n",
    "                                  'miRNA_and_gene_expression_and_dna_methylation_data'\n",
    "                                 ]\n",
    "                       }  \n",
    "\n",
    "omics_levels_map = {\n",
    "    'miRNA_data': \"SingleOmics\",\n",
    "    'dna_methylation_data': \"SingleOmics\",\n",
    "    'gene_expression_data': \"SingleOmics\",\n",
    "    'miRNA_and_gene_expression_data': \"DualOmics\",\n",
    "    'miRNA_and_dna_methylation_data': \"DualOmics\",\n",
    "    'gene_expression_and_dna_methylation_data': \"DualOmics\",\n",
    "    'miRNA_and_gene_expression_and_dna_methylation_data': \"TripleOmics\",\n",
    "    'metabolomics_data': \"SingleOmics\",\n",
    "    'proteomics_data': \"SingleOmics\",\n",
    "    'gene_expression_and_proteomics_data': \"DualOmics\",\n",
    "    'metabolomics_and_gene_expression_data': \"DualOmics\",\n",
    "    'metabolomics_and_proteomics_data': \"DualOmics\",\n",
    "    'metabolomics_and_gene_expression_and_proteomics_data': \"TripleOmics\"\n",
    "}\n",
    "\n",
    "omics_types_map = {\n",
    "    'miRNA_data': \"miRNA\",\n",
    "    'dna_methylation_data': \"Meth\",\n",
    "    'gene_expression_data': \"mRNA\",\n",
    "    \n",
    "    # Dual-omics\n",
    "    'miRNA_and_gene_expression_data': \"miRNA+mRNA\",\n",
    "    'miRNA_and_dna_methylation_data': \"miRNA+Meth\",\n",
    "    'gene_expression_and_dna_methylation_data': \"mRNA+Meth\",\n",
    "    'gene_expression_and_proteomics_data': \"mRNA+Prot\",\n",
    "    'metabolomics_and_gene_expression_data': \"mRNA+Metab\",\n",
    "    'metabolomics_and_proteomics_data': \"Metab+Prot\",\n",
    "    \n",
    "    # Triple-omics\n",
    "    'miRNA_and_gene_expression_and_dna_methylation_data': \"miRNA+mRNA+Meth\",\n",
    "    'metabolomics_and_gene_expression_and_proteomics_data': \"mRNA+Metab+Prot\",\n",
    "    \n",
    "    # Single other omics\n",
    "    'metabolomics_data': \"Metab\",\n",
    "    'proteomics_data': \"Prot\"\n",
    "}\n",
    "\n",
    "\n",
    "Cross_Validation_Results = pd.DataFrame()\n",
    "Biomarker_Validation_Results = pd.DataFrame()\n",
    "Cross_Validation_and_Biomarker_Validation_Results = pd.DataFrame()\n",
    "Selected_Biomarker_Panels = {\n",
    "    \"Feature\": [],\n",
    "    \"Cohort\": [],\n",
    "    \"OmicsLevel\": [],\n",
    "    \"OmicsTypes\": []\n",
    "}\n",
    "\n",
    "featureSelectors = ['MOGONET:Ranker',\n",
    " 'MORE:Ranker',\n",
    " 'boruta',\n",
    " 'elasticnet',\n",
    " 'geom.mean_rank',\n",
    " 'geom.mean_weight',\n",
    " 'lasso',\n",
    " 'lime',\n",
    " 'mannwhitneyu',\n",
    " 'max_weight',\n",
    " 'mean_rank',\n",
    " 'mean_weight',\n",
    " 'median_rank',\n",
    " 'median_weight',\n",
    " 'min_rank',\n",
    " 'mra_rank',\n",
    " 'randomforest_feature_importance',\n",
    " 'rf_permutation_feature_importance',\n",
    " 'ridge',\n",
    " 'rra_rank',\n",
    " 'shap',\n",
    " 'stuart_rank',\n",
    " 'svm_rfe',\n",
    " 't_test',\n",
    " 'ta_weight',\n",
    " 'xgb_feature_importance',\n",
    " 'xgb_permutation_feature_importance']\n",
    "\n",
    "for featureSelector in featureSelectors:\n",
    "    Selected_Biomarker_Panels[featureSelector] = []\n",
    "\n",
    "for dataset_name, experiment_list in experimental_designs.items():\n",
    "    print(f\"\\nWORKING ON {dataset_name} DATASET\")\n",
    "    df = pd.read_csv(f'../data/{dataset_name}/prepared/{experiment_list[-1]}.csv', index_col=0) \n",
    "    \n",
    "    for experiment_name in experiment_list:\n",
    "        print(f\"WORKING ON {experiment_name} EXPERIMENT\")\n",
    "        view_column_splits = get_view_column_splits(df) \n",
    "        cleaned_results = run_pipeline(dataset_name, experiment_name, view_column_splits) \n",
    "\n",
    "        added_attributes = pd.DataFrame({\n",
    "            \"Cohort\": [dataset_name],\n",
    "            \"OmicsLevel\": [omics_levels_map[experiment_name]],\n",
    "            \"OmicsType\": [omics_types_map[experiment_name]],\n",
    "        })\n",
    "\n",
    "        # Extend Cross_Validation_Results\n",
    "        added_attributes_extended = pd.concat([added_attributes] * cleaned_results['cv_performance'].shape[0], ignore_index=True)\n",
    "        Cross_Validation_Results_current =  pd.concat([added_attributes_extended,  cleaned_results['cv_performance']], axis = 1)\n",
    "        Cross_Validation_Results = pd.concat([Cross_Validation_Results, Cross_Validation_Results_current], axis = 0)\n",
    "\n",
    "        # Extend Biomarker_Validation_Results\n",
    "        if cleaned_results['val_results'].shape[0] > 0:\n",
    "            added_attributes_extended = pd.concat([added_attributes] * cleaned_results['val_results'].shape[0], ignore_index=True)\n",
    "            Biomarker_Validation_Results_current =  pd.concat([added_attributes_extended,  cleaned_results['val_results']], axis = 1)\n",
    "            Biomarker_Validation_Results = pd.concat([Biomarker_Validation_Results, Biomarker_Validation_Results_current], axis = 0)\n",
    "\n",
    "        # Extend Biomarker_Validation_Results\n",
    "        if cleaned_results['full_val_results'].shape[0] > 0:\n",
    "            added_attributes_extended = pd.concat([added_attributes] * cleaned_results['full_val_results'].shape[0], ignore_index=True)\n",
    "            Cross_Validation_and_Biomarker_Validation_Results_current =  pd.concat([added_attributes_extended,  cleaned_results['full_val_results']], axis = 1)\n",
    "            Cross_Validation_and_Biomarker_Validation_Results = pd.concat([Cross_Validation_and_Biomarker_Validation_Results, Cross_Validation_and_Biomarker_Validation_Results_current], axis = 0)\n",
    "            \n",
    "        ranked_matrix = make_rank_matrix(cleaned_results[\"ranks\"], fill_missing=None, one_based=True)\n",
    "\n",
    "        for feature in ranked_matrix.index:\n",
    "            Selected_Biomarker_Panels[\"Feature\"].append(feature)\n",
    "            Selected_Biomarker_Panels[\"Cohort\"].append(dataset_name)\n",
    "            Selected_Biomarker_Panels[\"OmicsLevel\"].append(omics_levels_map[experiment_name])\n",
    "            Selected_Biomarker_Panels[\"OmicsTypes\"].append(omics_types_map[experiment_name])\n",
    "            for featureSelector in featureSelectors:\n",
    "                Selected_Biomarker_Panels[featureSelector].append(ranked_matrix.loc[feature, featureSelector])\n",
    "                \n",
    "Selected_Biomarker_Panels = pd.DataFrame(Selected_Biomarker_Panels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b335e0b1-e7b7-4136-b794-a80cb810cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "Cross_Validation_Results.to_csv(f\"../BENCHMARKING/Cross_Validation_Results.csv\", index = False)\n",
    "Biomarker_Validation_Results.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
    "Biomarker_Validation_Results.to_csv(f\"../BENCHMARKING/Biomarker_Validation_Results.csv\", index = False)\n",
    "Cross_Validation_and_Biomarker_Validation_Results.drop(columns = [\"Unnamed: 0\"], inplace = True)\n",
    "Cross_Validation_and_Biomarker_Validation_Results.to_csv(f\"../BENCHMARKING/Cross_Validation_and_Biomarker_Validation_Results.csv\", index = False)\n",
    "Selected_Biomarker_Panels.to_csv(f\"../BENCHMARKING/Selected_Biomarker_Panels.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
